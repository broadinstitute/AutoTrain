{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.laplace import Laplace\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import  tqdm\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from functools import partial \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:3\")\n",
    "DATA_ROOT = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Model Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(transforms):\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=DATA_ROOT / 'cifar-10-data', train=True,\n",
    "                                            download=True, transform=transforms)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=DATA_ROOT / 'cifar-10-data', train=False,\n",
    "                                           download=True, transform=transforms)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "convnet = ConvNet()\n",
    "convnet.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet.fc = nn.Linear(512, len(CLASSES))\n",
    "resnet.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thresholdout:\n",
    "    def __init__(self, train, holdout, tolerance=0.01/4, scale_factor=4, keep_log=True):\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        self.laplace_eps = Laplace(torch.tensor([0.0]), torch.tensor([2*self.tolerance]))\n",
    "        self.laplace_gamma = Laplace(torch.tensor([0.0]), torch.tensor([4*self.tolerance]))\n",
    "        self.laplace_eta = Laplace(torch.tensor([0.0]), torch.tensor([8*self.tolerance]))\n",
    "\n",
    "        self.train = train\n",
    "        self.holdout = holdout\n",
    "        \n",
    "        self.T = 4*tolerance + self.noise(self.laplace_gamma)\n",
    "        # self.budget = ???\n",
    "        \n",
    "        self.keep_log = keep_log\n",
    "        if keep_log:\n",
    "            self.log = pd.DataFrame(columns=['GlobStep', 'threshold', 'delta', 'phi_train', \\\n",
    "                                             'phi_holdout', 'estimate', 'overfit'])\n",
    "        \n",
    "        \n",
    "    def noise(self, dist):\n",
    "        return dist.sample().item()\n",
    "        \n",
    "    def verify_statistic(self, phi, glob_step=None):\n",
    "        \"\"\"\n",
    "            - phi(dataset) -> statistic: \n",
    "              function returns the average of some statistic\n",
    "        \"\"\"\n",
    "        \n",
    "        train_val = phi(self.train)\n",
    "        holdout_val = phi(self.holdout)\n",
    "                \n",
    "        delta = abs(train_val - holdout_val)\n",
    "        thresh = self.T + self.noise(self.laplace_eta)\n",
    "        \n",
    "        if delta > thresh:\n",
    "            self.T += self.noise(self.laplace_gamma)\n",
    "            estimate = holdout_val + self.noise(self.laplace_eps)\n",
    "        else:\n",
    "            estimate = train_val\n",
    "            \n",
    "        if self.keep_log:\n",
    "            if glob_step is None: \n",
    "                raise ValueException('please provide glob step if logging is on')\n",
    "            self.log.loc[len(self.log)] = [glob_step, thresh, delta, train_val, holdout_val, estimate, delta > thresh]\n",
    "            \n",
    "        return estimate\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, data_loader): \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data[0].to(DEVICE), data[1]\n",
    "            outputs = model(images).cpu()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs=20, sample_step=1000):\n",
    "    \n",
    "    trainloader, testloader = data\n",
    "    tout = Thresholdout(trainloader, testloader, keep_log=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    loss_history = []\n",
    "    glob_step = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            loss_history += [loss.item()]\n",
    "            glob_step += 1\n",
    "            \n",
    "            if i % sample_step == 0 and i:\n",
    "                acc_val = tout.verify_statistic(partial(test_accuracy, model), glob_step)\n",
    "                print(f'[{epoch+1}, step::{i}] loss [{running_loss / sample_step :.3f}] accuracy [{acc_val:.3f}]')\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    return loss_history, tout\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6f7eecf8034dfba729cb2284841207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, step::1000] loss [1.600] accuracy [0.500]\n",
      "[1, step::2000] loss [1.198] accuracy [0.626]\n",
      "[1, step::3000] loss [1.002] accuracy [0.664]\n",
      "[2, step::1000] loss [0.850] accuracy [0.732]\n",
      "[2, step::2000] loss [0.791] accuracy [0.724]\n",
      "[2, step::3000] loss [0.737] accuracy [0.741]\n",
      "[3, step::1000] loss [0.636] accuracy [0.757]\n",
      "[3, step::2000] loss [0.620] accuracy [0.780]\n",
      "[3, step::3000] loss [0.610] accuracy [0.792]\n",
      "[4, step::1000] loss [0.498] accuracy [0.801]\n",
      "[4, step::2000] loss [0.495] accuracy [0.799]\n",
      "[4, step::3000] loss [0.482] accuracy [0.809]\n",
      "[5, step::1000] loss [0.381] accuracy [0.809]\n",
      "[5, step::2000] loss [0.390] accuracy [0.813]\n",
      "[5, step::3000] loss [0.403] accuracy [0.793]\n",
      "[6, step::1000] loss [0.290] accuracy [0.816]\n",
      "[6, step::2000] loss [0.306] accuracy [0.817]\n",
      "[6, step::3000] loss [0.308] accuracy [0.816]\n",
      "[7, step::1000] loss [0.201] accuracy [0.812]\n",
      "[7, step::2000] loss [0.244] accuracy [0.810]\n",
      "[7, step::3000] loss [0.242] accuracy [0.830]\n",
      "[8, step::1000] loss [0.147] accuracy [0.813]\n",
      "[8, step::2000] loss [0.174] accuracy [0.816]\n",
      "[8, step::3000] loss [0.196] accuracy [0.832]\n",
      "[9, step::1000] loss [0.103] accuracy [0.823]\n",
      "[9, step::2000] loss [0.142] accuracy [0.821]\n",
      "[9, step::3000] loss [0.146] accuracy [0.808]\n",
      "[10, step::1000] loss [0.089] accuracy [0.824]\n",
      "[10, step::2000] loss [0.107] accuracy [0.822]\n",
      "[10, step::3000] loss [0.124] accuracy [0.829]\n",
      "[11, step::1000] loss [0.078] accuracy [0.815]\n",
      "[11, step::2000] loss [0.092] accuracy [0.820]\n",
      "[11, step::3000] loss [0.100] accuracy [0.820]\n",
      "[12, step::1000] loss [0.067] accuracy [0.814]\n",
      "[12, step::2000] loss [0.087] accuracy [0.821]\n",
      "[12, step::3000] loss [0.080] accuracy [0.827]\n",
      "[13, step::1000] loss [0.056] accuracy [0.821]\n",
      "[13, step::2000] loss [0.079] accuracy [0.826]\n",
      "[13, step::3000] loss [0.082] accuracy [0.829]\n",
      "[14, step::1000] loss [0.046] accuracy [0.828]\n",
      "[14, step::2000] loss [0.068] accuracy [0.831]\n",
      "[14, step::3000] loss [0.070] accuracy [0.822]\n",
      "[15, step::1000] loss [0.048] accuracy [0.823]\n",
      "[15, step::2000] loss [0.059] accuracy [0.833]\n",
      "[15, step::3000] loss [0.064] accuracy [0.829]\n"
     ]
    }
   ],
   "source": [
    "resnet_normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "resnet_transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor(), resnet_normalize])\n",
    "\n",
    "resnet_data = make_data(resnet_transform)\n",
    "resnet_history, resnet_tout = train(resnet, resnet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resnet-outcome.pkl','wb') as fp:\n",
    "    pkl.dump((resnet_history, resnet_tout), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "\n",
    "import autotrain\n",
    "import autotrain.gym_env \n",
    "import autotrain.agent.replay_memory as replay_memory\n",
    "\n",
    "# if gpu is to be used\n",
    "DEVICE = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent import\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import autotrain.agent.dqn as dqn\n",
    "import autotrain.agent.replay_memory as replay_memory\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_MLP(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, outputs),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 20\n",
    "\n",
    "\n",
    "\n",
    "class AutoTrainAgent: \n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        \n",
    "        self.env = env\n",
    "        self.device = device\n",
    "\n",
    "        self.observation_dim = env.observation_space_dim\n",
    "        \n",
    "        # Get number of actions from gym action space\n",
    "        self.n_actions = env.action_space_dim\n",
    "\n",
    "        self.policy_net = dqn.DQN_MLP(self.observation_dim, self.n_actions).to(device)\n",
    "        self.target_net = dqn.DQN_MLP(self.observation_dim, self.n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.memory = replay_memory.ReplayMemory(10000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                \n",
    "                if len(state.shape) == 1:\n",
    "                    state = state.view(1, -1)\n",
    "                print(f'[ATA] state dimensions: ', state.shape)\n",
    "                actions = self.policy_net(state)\n",
    "                \n",
    "                return actions.max(-1)[1].view(-1, 1)\n",
    "        else:\n",
    "            print('[ATA] exploration policy enacted')\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = replay_memory.Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.stack([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.cat(batch.action).view(BATCH_SIZE, 1)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        print(f'[ATA] batch shapes: [{state_batch.shape} {action_batch.shape} {reward_batch.shape}]')\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        print('[ATA] DQN optimisation step complete ')\n",
    "        \n",
    "    def _preproc_state(self, state):\n",
    "        return torch.FloatTensor(np.concatenate(state, axis=0)).to(self.device)\n",
    "        \n",
    "    def episode(self, i_episode):\n",
    "        # Initialize the environment and state\n",
    "        global RL_PROGRESS_PATH\n",
    "\n",
    "        state = self._preproc_state(self.env.reset())\n",
    "\n",
    "        for t in count():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Select and perform an action\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action.item())\n",
    "            next_state = self._preproc_state(next_state) if not done else next_state  # for MLP, only one dim\n",
    "            \n",
    "            reward = torch.FloatTensor([reward]).to(self.device)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            self.optimize_model()\n",
    "            print(f'[ATA episode {i_episode}]: took [{time.time() - start_time:.1f}] seconds for one full step')\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        print(f'[ATA] replay buffer length: [{len(self.memory)}]')\n",
    "        savepath = RL_PROGRESS_PATH / f'{i_episode}_episode'\n",
    "        savepath.mkdir()\n",
    "        env.save_env(savepath)\n",
    "        torch.save(self.policy_net.state_dict(), savepath / 'policy_net.ckpt')\n",
    "        torch.save(self.target_net.state_dict(), savepath / 'target_net.ckpt')\n",
    "        self.memory.save(savepath / 'replay_buffer.pkl')\n",
    "\n",
    "        return t\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Data Prep - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('../data')\n",
    "DATA_SPLIT = 0.6\n",
    "\n",
    "ENV_PATH = Path('./results/autotrain-run-risa-cifar-clone')\n",
    "ENV_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "RL_PROGRESS_PATH = Path('./results/') \n",
    "RL_PROGRESS_PATH.mkdir(exist_ok=True)\n",
    "RL_PROGRESS_PATH /= 'RISA_proto_1_cifar_clone'\n",
    "RL_PROGRESS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(RL_PROGRESS_PATH.iterdir()):\n",
    "    print('[WARNING!!]: save dir is not empty; agent will not save if folder exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = CLASSES = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def reduceds(ds, pct_cap, no_signal=False):\n",
    "    X, Y = ds.data, ds.targets\n",
    "    \n",
    "    if pct_cap:\n",
    "        cap = int(pct_cap*len(X))\n",
    "        X, Y = X[:cap], Y[:cap]\n",
    "        \n",
    "    \n",
    "    if no_signal:\n",
    "        print('suffling labels')\n",
    "        np.random.shuffle(Y)\n",
    "    \n",
    "    ds.data, ds.targets = X, Y\n",
    "\n",
    "def get_dataset(tfms, no_signal=False, pct_cap=None):\n",
    "    train = torchvision.datasets.CIFAR10(root=DATA_ROOT / 'cifar-10-data', train=True,\n",
    "                                        download=True, transform=tfms)\n",
    "\n",
    "    holdout = torchvision.datasets.CIFAR10(root=DATA_ROOT / 'cifar-10-data', train=False,\n",
    "                                           download=True, transform=tfms)\n",
    "        \n",
    "    # train.data, train.targets = train.data.numpy(),train.targets.numpy()\n",
    "    # holdout.data, holdout.targets = holdout.data.numpy(),  holdout.targets.numpy()\n",
    "    \n",
    "    reduceds(train, pct_cap, no_signal)\n",
    "    \n",
    "    print(f'length of trainset: [{len(train)}]; len of holdout: [{len(holdout)}]')\n",
    "    \n",
    "    return train, holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "length of trainset: [10000]; len of holdout: [10000]\n"
     ]
    }
   ],
   "source": [
    "TFMS = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train, holdout = get_dataset(TFMS, pct_cap=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data: torch.utils.data.DataLoader, model: nn.Module): # phi\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data,total=len(data)):\n",
    "            images, labels = batch[0].to(DEVICE), batch[1]\n",
    "            outputs = model(images).cpu()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Network Definition - SimpleConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1) # 1->3 for CIFAR\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.fc2 = nn.Linear(128, len(CLASSES))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTrain Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REWIND_DIM = 3\n",
    "CLF_BS = 16\n",
    "SAMPLE = 50 # sampling interval\n",
    "BATCH_UPDATES = len(train) // CLF_BS # half an epoch\n",
    "LOSS_DIM = BATCH_UPDATES // SAMPLE\n",
    "(LOSS_DIM + 2 )* 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time_step:0]  initialised backbone parameters & optimizer\n",
      "[time_step:0]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc599bcd33704e37854809d8a650e32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaba365e99a476d861ace890642e747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:0]  initialised phi value: done\n",
      "[time_step:0]  added observation\n",
      "[time_step:0]  environment initialised : AutoTrainEnvironment with the following parameters:\n",
      "                        lr_init=0.0003, inter_reward=1, \n",
      "                        H=3, K=12, T=625\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('AutoTrain-v0') # TODO loss func nll_loss\n",
    "\n",
    "ob = env.init(backbone=clf,  phi=accuracy, savedir=ENV_PATH,\n",
    "         trnds=train, valds=holdout,\n",
    "         T=BATCH_UPDATES, H=REWIND_DIM, S=SAMPLE, lr_init=3e-4, \n",
    "         inter_reward=1, final_reward_scale=100,\n",
    "         horizon=50, criterion=F.nll_loss,\n",
    "         num_workers=4, bs=CLF_BS, v=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2, figsize=(12,6))\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time_step:0]  initialised backbone parameters & optimizer\n",
      "[time_step:0]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cd4a0459114744817f130effd600fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398e3c75a1b4400cbb3f9a9f94b94446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:0]  initialised phi value: done\n",
      "[time_step:0]  added observation\n",
      "[time_step:0]  environment initialised : AutoTrainEnvironment with the following parameters:\n",
      "                        lr_init=0.0003, inter_reward=1, \n",
      "                        H=3, K=12, T=625\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:0]  action [4] received\n",
      "[time_step:0]  training loop started ...\n",
      "[time_step:1]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7e39d786b04f11a01bbdbd238669ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9629c2e17c194908996863a7f9c3ea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:1]  added observation\n",
      "[time_step:1]  reward at the end of time step is [1]\n",
      "[ATA episode 0]: took [7.4] seconds for one full step\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:1]  action [2] received\n",
      "[time_step:1]  training loop started ...\n",
      "[time_step:2]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6fd79b4ce644ccbe6ef42145d14e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45722c59318a422fb6c7949deb34a02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:2]  added observation\n",
      "[time_step:2]  reward at the end of time step is [1]\n",
      "[ATA episode 0]: took [7.4] seconds for one full step\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:2]  action [4] received\n",
      "[time_step:2]  training loop started ...\n",
      "[time_step:3]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cf69c212114e31b7cf8eec2c184214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039c84afa3cc4c308d46624c0905f733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:3]  added observation\n",
      "[time_step:3]  reward at the end of time step is [1]\n",
      "[ATA episode 0]: took [7.3] seconds for one full step\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:3]  action [5] received\n",
      "[time_step:3]  received RE-INIT signal\n",
      "[time_step:3]  initialised backbone parameters & optimizer\n",
      "[time_step:3]  training loop started ...\n",
      "[time_step:4]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf15e646c3a499895803b70653f3f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7957ca0c8e44c03bb5f1e9edb434947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train an agent\n",
    "\n",
    "agent = AutoTrainAgent(env, DEVICE)\n",
    "\n",
    "num_episodes = 300\n",
    "for i_episode in range(num_episodes):\n",
    "    t = agent.episode(i_episode)\n",
    "    episode_durations.append(t + 1)\n",
    "    plot_durations()\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "print('Complete')\n",
    "cpenv.env.render()\n",
    "cpenv.env.close()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.logmdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

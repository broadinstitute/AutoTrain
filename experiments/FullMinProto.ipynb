{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "\n",
    "import autotrain\n",
    "import autotrain.gym_env \n",
    "import autotrain.agent.replay_memory as replay_memory\n",
    "\n",
    "# if gpu is to be used\n",
    "DEVICE = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent import\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import autotrain.agent.dqn as dqn\n",
    "import autotrain.agent.replay_memory as replay_memory\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_MLP(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, outputs),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 2\n",
    "\n",
    "\n",
    "\n",
    "class AutoTrainAgent: \n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        \n",
    "        self.env = env\n",
    "        self.device = device\n",
    "\n",
    "        self.observation_dim = env.observation_space_dim\n",
    "        \n",
    "        # Get number of actions from gym action space\n",
    "        self.n_actions = env.action_space_dim\n",
    "\n",
    "        self.policy_net = dqn.DQN_MLP(self.observation_dim, self.n_actions).to(device)\n",
    "        self.target_net = dqn.DQN_MLP(self.observation_dim, self.n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.memory = replay_memory.ReplayMemory(10000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                \n",
    "                if len(state.shape) == 1:\n",
    "                    state = state.view(1, -1)\n",
    "                print(f'[ATA] state dimensions: ', state.shape)\n",
    "                actions = self.policy_net(state)\n",
    "                \n",
    "                return actions.max(-1)[1].view(-1, 1)\n",
    "        else:\n",
    "            print('[ATA] exploration policy enacted')\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = replay_memory.Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def _preproc_state(self, state):\n",
    "        return torch.FloatTensor(np.concatenate(state, axis=0)).to(self.device)\n",
    "        \n",
    "    def episode(self, i_episode):\n",
    "        # Initialize the environment and state\n",
    "        global RL_PROGRESS_PATH\n",
    "\n",
    "        state = self._preproc_state(self.env.reset())\n",
    "\n",
    "        for t in count():\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action.item())\n",
    "            next_state = self._preproc_state(next_state) if not done else next_state  # for MLP, only one dim\n",
    "            \n",
    "            reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            self.optimize_model()\n",
    "            print(f'[ATA episode {i_episode}]: took [{time.time() - start_time:.1f}] seconds for one full step')\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        print(f'[ATA] replay buffer length: [{len(self.memory)}]')\n",
    "        savepath = RL_PROGRESS_PATH / f'{i_episode}_episode'\n",
    "        savepath.mkdir()\n",
    "        env.save_env(savepath)\n",
    "        torch.save(self.policy_net.state_dict(), savepath / 'policy_net.ckpt')\n",
    "        torch.save(self.target_net.state_dict(), savepath / 'target_net.ckpt')\n",
    "        self.memory.save(savepath / 'replay_buffer.pkl')\n",
    "\n",
    "        return t\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Data Prep - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('../data')\n",
    "DATA_SPLIT = 0.6\n",
    "\n",
    "ENV_PATH = Path('./results/autotrain-run')\n",
    "ENV_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "RL_PROGRESS_PATH = Path('./results/') \n",
    "RL_PROGRESS_PATH.mkdir(exist_ok=True)\n",
    "RL_PROGRESS_PATH /= 'full_min_proto_0'\n",
    "RL_PROGRESS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = range(10)\n",
    "\n",
    "def reduceds(ds, pct_cap, no_signal=False):\n",
    "    X, Y = ds.data, ds.targets\n",
    "    \n",
    "    if pct_cap:\n",
    "        cap = int(pct_cap*len(X))\n",
    "        X, Y = X[:cap], Y[:cap]\n",
    "        \n",
    "    \n",
    "    if no_signal:\n",
    "        print('suffling labels')\n",
    "        np.random.shuffle(Y)\n",
    "    \n",
    "    ds.data, ds.targets = X, Y\n",
    "\n",
    "def get_dataset(tfms, no_signal=False, pct_cap=None):\n",
    "    train = torchvision.datasets.MNIST(root=DATA_ROOT / 'mnist-data', train=True,\n",
    "                                        download=True, transform=tfms)\n",
    "\n",
    "    holdout = torchvision.datasets.MNIST(root=DATA_ROOT / 'mnist-data', train=False,\n",
    "                                           download=True, transform=tfms)\n",
    "        \n",
    "    # train.data, train.targets = train.data.numpy(),train.targets.numpy()\n",
    "    # holdout.data, holdout.targets = holdout.data.numpy(),  holdout.targets.numpy()\n",
    "    \n",
    "    reduceds(train, pct_cap, no_signal)\n",
    "    \n",
    "    print(f'length of trainset: [{len(train)}]; len of holdout: [{len(holdout)}]')\n",
    "    \n",
    "    return train, holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of trainset: [12000]; len of holdout: [10000]\n"
     ]
    }
   ],
   "source": [
    "TFMS = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "train, holdout = get_dataset(TFMS, pct_cap=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data: torch.utils.data.DataLoader, model: nn.Module): # phi\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data,total=len(data)):\n",
    "            images, labels = batch[0].to(DEVICE), batch[1]\n",
    "            outputs = model(images).cpu()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Network Definition - SimpleConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTrain Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REWIND_DIM = 5\n",
    "CLF_BS = 16\n",
    "SAMPLE = 50 # sampling interval\n",
    "BATCH_UPDATES = len(train) // CLF_BS * 3 # three epochs\n",
    "LOSS_DIM = BATCH_UPDATES // SAMPLE\n",
    "(LOSS_DIM + 2 )* 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time_step:0]  initialised backbone parameters & optimizer\n",
      "[time_step:0]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aee2d8e2c8847fc99eea116925c9e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17f182930c54311952ab2699f60b72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:0]  initialised phi value: done\n",
      "[time_step:0]  added observation\n",
      "[time_step:0]  environment initialised : AutoTrainEnvironment with the following parameters:\n",
      "                        lr_init=0.0003, inter_reward=0.05, H=5, K=45, T=2250\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('AutoTrain-v0') # TODO loss func nll_loss\n",
    "\n",
    "ob = env.init(backbone=clf,  phi=accuracy, savedir=ENV_PATH,\n",
    "         trnds=train, valds=holdout, \n",
    "         T=BATCH_UPDATES, H=REWIND_DIM, S=SAMPLE, lr_init=3e-4, inter_reward=0.05,\n",
    "         horizon=50, criterion=F.nll_loss,\n",
    "         num_workers=4, bs=CLF_BS, v=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2, figsize=(12,6))\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time_step:0]  initialised backbone parameters & optimizer\n",
      "[time_step:0]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a301883f5da4f40a02953c1feb9ca2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58290606944ff69a6837729fa14272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:0]  initialised phi value: done\n",
      "[time_step:0]  added observation\n",
      "[time_step:0]  environment initialised : AutoTrainEnvironment with the following parameters:\n",
      "                        lr_init=0.0003, inter_reward=0.05, H=5, K=45, T=2250\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:0]  action [8] recieved\n",
      "[time_step:0]  recieved RE-INIT signal or rewind_steps[3] > len(ll)\n",
      "[time_step:0]  initialised backbone parameters & optimizer\n",
      "[time_step:0]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92836468f8854cd7a5c2609804f8e477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4b10d71aba4a939c7f22f4bad5311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:0]  initialised phi value: done\n",
      "[time_step:0]  added observation\n",
      "[time_step:0]  training loop started ...\n",
      "[time_step:1]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf0a0abe5e64016b1fd4c70d73961fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c2f6fd70294640913fc0cd087988bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:1]  added observation\n",
      "[time_step:1]  reward at the end of time step is [0.05]\n",
      "[ATA episode 1]: took [15.7] seconds for one full step\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:1]  action [5] recieved\n",
      "[time_step:1]  training loop started ...\n",
      "[time_step:2]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3104f3fddb504fd3b1ddec78b705ab77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119403463e2f4d61907a2c2300d2668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:2]  added observation\n",
      "[time_step:2]  reward at the end of time step is [0.05]\n",
      "[ATA episode 1]: took [12.8] seconds for one full step\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:2]  action [10] recieved\n",
      "[time_step:2]  increased lr by 10% -> [lr:0.00033]\n",
      "[time_step:2]  training loop started ...\n",
      "[time_step:3]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc35fcd4240040278f1e100185f6b8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479214a20ae5408b9a16c6306cdbe1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:3]  added observation\n",
      "[time_step:3]  reward at the end of time step is [0.05]\n",
      "[ATA episode 1]: took [12.5] seconds for one full step\n",
      "[ATA] exploration policy enacted\n",
      "[time_step:3]  action [4] recieved\n",
      "[time_step:3]  decreased lr by 10% -> [lr:0.000297]\n",
      "[time_step:3]  recieved RE-INIT signal or rewind_steps[4] > len(ll)\n",
      "[time_step:3]  initialised backbone parameters & optimizer\n",
      "[time_step:3]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83f63f9e31a40d8b06075ad6246c0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2374da8eb5b4db795ff78be5a81d88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:3]  initialised phi value: done\n",
      "[time_step:3]  added observation\n",
      "[time_step:3]  training loop started ...\n",
      "[time_step:4]  training loop done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762ce580cbcd4eb79fad950fd59775c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3e2a2b634f4b9bbb97f5c0e1ce164f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train an agent\n",
    "\n",
    "agent = AutoTrainAgent(env, DEVICE)\n",
    "\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    t = agent.episode(i_episode)\n",
    "    episode_durations.append(t + 1)\n",
    "    plot_durations()\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "print('Complete')\n",
    "cpenv.env.render()\n",
    "cpenv.env.close()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

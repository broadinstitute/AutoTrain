{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-skenjeye@broadinst-05974/AutoTrain/experiments\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/jupyter-skenjeye%40broadinst-05974/AutoTrain/autotrain\n",
      "Requirement already satisfied: gym in /home/jupyter-skenjeye@broadinst-05974/.local/lib/python3.7/site-packages (from autotrain==0.0.1) (0.17.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/jupyter-skenjeye@broadinst-05974/.local/lib/python3.7/site-packages (from gym->autotrain==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/tljh/user/lib/python3.7/site-packages (from gym->autotrain==0.0.1) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/tljh/user/lib/python3.7/site-packages (from gym->autotrain==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /opt/tljh/user/lib/python3.7/site-packages (from gym->autotrain==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: future in /opt/tljh/user/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->autotrain==0.0.1) (0.18.2)\n",
      "Installing collected packages: autotrain\n",
      "  Attempting uninstall: autotrain\n",
      "    Found existing installation: autotrain 0.0.1\n",
      "    Uninstalling autotrain-0.0.1:\n",
      "      Successfully uninstalled autotrain-0.0.1\n",
      "  Running setup.py develop for autotrain\n",
      "Successfully installed autotrain\n"
     ]
    }
   ],
   "source": [
    "# buggy, some kernel issue\n",
    "!python -m pip install -e ../autotrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autotrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb0d907fc238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautotrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautotrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautotrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autotrain'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "\n",
    "import autotrain\n",
    "import autotrain.gym_env \n",
    "import autotrain.agent.replay_memory as replay_memory\n",
    "\n",
    "# if gpu is to be used\n",
    "DEVICE = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent import\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import autotrain.agent.dqn as dqn\n",
    "import autotrain.agent.replay_memory as replay_memory\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_MLP(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, outputs),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\"\"\"\n",
    "agent TODO:\n",
    "     - saving agent state\n",
    "\"\"\"\n",
    "\n",
    "class AutoTrainAgent: \n",
    "\n",
    "    def __init__(self, env, device):\n",
    "        \n",
    "        self.env = env\n",
    "        self.device = device\n",
    "\n",
    "        self.observation_dim = env.observation_space_dim\n",
    "        \n",
    "        # Get number of actions from gym action space\n",
    "        self.n_actions = env.action_space_dim\n",
    "\n",
    "        self.policy_net = dqn.DQN_MLP(self.observation_dim, self.n_actions).to(device)\n",
    "        self.target_net = dqn.DQN_MLP(self.observation_dim, self.n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.memory = replay_memory.ReplayMemory(10000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = replay_memory.Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def episode(self, i_episode):\n",
    "        # Initialize the environment and state\n",
    "\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        for t in count():\n",
    "            # Select and perform an action\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action.item())\n",
    "            next_state = np.concatenate(next_state, axis=0) # for MLP, only one dim\n",
    "            \n",
    "            reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            self.optimize_model()\n",
    "            if done:\n",
    "                break\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return t\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Data Prep - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('./data')\n",
    "DATA_SPLIT = 0.6\n",
    "\n",
    "ENV_PATH = Path('./autotrain-run')\n",
    "ENV_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = range(10)\n",
    "\n",
    "def splitds(train, test, no_signal=False, pct_cap=None):\n",
    "    X = np.concatenate((train.data, test.data), axis=0)\n",
    "    Y = list(train.targets) + list(test.targets)\n",
    "    \n",
    "    if pct_cap:\n",
    "        cap = int(pct_cap*len(X))\n",
    "        X, Y = X[:cap], Y[:cap]\n",
    "        \n",
    "    \n",
    "    if no_signal:\n",
    "        print('suffling labels')\n",
    "        np.random.shuffle(Y)\n",
    "    \n",
    "    split_id = int(len(X) * DATA_SPLIT)\n",
    "    train.data, train.targets = X[:split_id], Y[:split_id]\n",
    "    test.data, test.targets = X[split_id:], Y[split_id:]\n",
    "\n",
    "def get_dataset(tfms, no_signal=False, pct_cap=None):\n",
    "    train = torchvision.datasets.MNIST(root=DATA_ROOT / 'mnist-data', train=True,\n",
    "                                        download=True, transform=tfms)\n",
    "\n",
    "    holdout = torchvision.datasets.MNIST(root=DATA_ROOT / 'mnist-data', train=False,\n",
    "                                           download=True, transform=tfms)\n",
    "        \n",
    "    # splitds(train, holdout, no_signal, pct_cap)\n",
    "    \n",
    "    print(f'length of trainset: [{len(train)}]; len of holdout: [{len(holdout)}]')\n",
    "    \n",
    "    return train, holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of trainset: [60000]; len of holdout: [10000]\n"
     ]
    }
   ],
   "source": [
    "TFMS = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "train, holdout = get_dataset(TFMS, pct_cap=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data: torch.utils.data.DataLoader, model: nn.Module): # phi\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data,total=len(data)):\n",
    "            images, labels = batch[0].to(DEVICE), batch[1]\n",
    "            outputs = model(images).cpu()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Network Definition - SimpleConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTrain Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REWIND_DIM = 5\n",
    "CLF_BS = 16\n",
    "SAMPLE = 20 # sampling interval\n",
    "BATCH_UPDATES = len(train) // CLF_BS * 3 # three epochs\n",
    "LOSS_DIM = BATCH_UPDATES // SAMPLE\n",
    "LOSS_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time_step:0]  initialised backbone parameters & optimizer\n",
      "[time_step:0]  initialised phi value: started ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b9eedb57df4fda8a30054fdd70aeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f5a180839c4af9a58dc92be052773b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[time_step:0]  initialised phi value: done\n",
      "[time_step:0]  added observation\n",
      "[time_step:0]  environment initialised : AutoTrainEnvironment with the following parameters:\n",
      "                        lr_init=0.0003, inter_reward=0.05, H=5, K=562, T=11250\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('AutoTrain-v0')\n",
    "\n",
    "ob = env.init(backbone=clf,  phi=accuracy, savedir=ENV_PATH,\n",
    "         trnds=train, valds=holdout, \n",
    "         T=BATCH_UPDATES, H=REWIND_DIM, S=SAMPLE, lr_init=3e-4, inter_reward=0.05,\n",
    "         num_workers=4, bs=CLF_BS, v=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2, figsize=(12,6))\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an agent\n",
    "\n",
    "agent = Agent(env, DEVICE)\n",
    "\n",
    "num_episodes = 5000\n",
    "for i_episode in range(num_episodes):\n",
    "    t = agent.episode(i_episode)\n",
    "    episode_durations.append(t + 1)\n",
    "    plot_durations()\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "print('Complete')\n",
    "cpenv.env.render()\n",
    "cpenv.env.close()\n",
    "plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
